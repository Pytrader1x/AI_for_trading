{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN LSTM Project 6 stocks sentiment analysis of market participants\n",
    "\n",
    "### Objective:\n",
    "\n",
    "#### Train an LSTM model to accurately predict sentiment of market participants through the proxy of \"stock Twits\" (https://stocktwits.com)\n",
    "\n",
    "\n",
    "### Once the model is trained save the most accurate epoch of that model and use model.eval() to efficienty perform inference on a live stream of twits.\n",
    "\n",
    "### This inference will be applied and backtested over time in order to generate an alpha factor and or risk factor model to be continued next.\n",
    "\n",
    "### This LSTM inference will be incorporated into a multi alpha factor model next using a Neural network to combine the multiple factors together.\n",
    "\n",
    "\n",
    "### Trained on an 80/ 20 Split of 1.58M market participant messages\n",
    "\n",
    "### Load Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Pytrader1x/AI_for_trading/blob/master/Screenshot%202019-09-10%20at%2020.58.08.png?raw=true \"Logo Title Text 1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Pytrader1x/AI_for_trading/blob/master/Screenshot%202019-09-10%20at%2020.57.55.png?raw=true \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will be using an LSTM inorder that the model inherently retains predicitive information from previous sequences of tokenised words\n",
    "\n",
    "## The LSTM will combine Short and long term memory two seperate a one layer networks \"the forget gate\" to establish what to remove from LTM and the \"Remember gate\" to establish what to keep in short term memory\n",
    "\n",
    "## This will allow the network to retain context from many sequences ago."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Pytrader1x/AI_for_trading/blob/master/Screenshot%202019-09-10%20at%2020.58.08.png?raw=true \"Logo Title Text 1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Pytrader1x/AI_for_trading/blob/master/image.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "![alt text](https://github.com/Pytrader1x/AI-Overwatch/blob/master/Picture%209.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/89/2dbdf5caecd3b6ccd26546971cf38d0527b76bf63d34d06150f4be2b9062/comet_ml-2.0.12-py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 4.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting comet-git-pure>=0.19.11 (from comet_ml)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/ae/a3d9b28e1d909bddbdba8ef7c331542614f3bf9fbba31bd4380c3294c569/comet_git_pure-0.19.11-py3-none-any.whl (383kB)\n",
      "\u001b[K    100% |████████████████████████████████| 389kB 25.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=2.6.0 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from comet_ml) (1.11.0)\n",
      "Collecting websocket-client>=0.55.0 (from comet_ml)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 19.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-ml-py3>=7.352.0 (from comet_ml)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/64/cce82bddb80c0b0f5c703bbdafa94bfb69a1c5ad7a79cff00b482468f0d3/nvidia-ml-py3-7.352.0.tar.gz\n",
      "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\" (from comet_ml)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.18.4)\n",
      "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/56/201c4d13c37b6fb0cb5dbf1d026a2fec14fd151fd4f3f1dc1144d6273fd3/wurlitzer-1.0.3-py2.py3-none-any.whl\n",
      "Collecting netifaces>=0.10.7 (from comet_ml)\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting urllib3>=1.23 (from comet-git-pure>=0.19.11->comet_ml)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 22.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.6/site-packages (from comet-git-pure>=0.19.11->comet_ml) (2019.6.16)\n",
      "Collecting configobj; extra == \"ini\" (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml)\n",
      "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (2.6)\n",
      "Building wheels for collected packages: nvidia-ml-py3, configobj\n",
      "  Running setup.py bdist_wheel for nvidia-ml-py3 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e4/1d/06/640c93f5270d67d0247f30be91f232700d19023f9e66d735c7\n",
      "  Running setup.py bdist_wheel for configobj ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
      "Successfully built nvidia-ml-py3 configobj\n",
      "\u001b[31mrequests 2.18.4 has requirement urllib3<1.23,>=1.21.1, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mbotocore 1.12.7 has requirement urllib3<1.24,>=1.20, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: urllib3, comet-git-pure, websocket-client, nvidia-ml-py3, configobj, everett, wurlitzer, netifaces, comet-ml\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "Successfully installed comet-git-pure-0.19.11 comet-ml-2.0.12 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 nvidia-ml-py3-7.352.0 urllib3-1.25.3 websocket-client-0.56.0 wurlitzer-1.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Optional metrics COMET\n",
    "\n",
    "#Optional extra metrics\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "For this project, you'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. You'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "We've collected a bunch of twits, then hand labeled the sentiment of each. To capture the degree of sentiment, we'll use a five-point scale: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. You'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "The first thing we should to do, is load the data.\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n",
    "\n",
    "\n",
    "To see what the data look like by printing the first 10 twits from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "print(len(twits['data']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "With our data in hand we need to preprocess our text. These twits are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the twit itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every twit, so we should remove them. This twit also has the `@google` username, again not providing sentiment information, so we should also remove it. We also see a URL `http://t.co/sptHOAh8`. Let's remove these too.\n",
    "\n",
    "The easiest way to remove specific words or phrases is with regex using the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'http\\S+',' ', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(r'\\$\\S+', ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(r'\\@\\S+', ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split(' ')\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [wnl.lemmatize(t) for t in tokens if len(t) > 1]\n",
    "     \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converts the list into text using strings\n",
    "# def convert(list):\n",
    "    \n",
    "#     s = [str(i) for i in list]\n",
    "#     result = \" \".join(s)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# messages_text = convert(messages)\n",
    "# messages_text[0:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement\n",
    "\n",
    "\n",
    "\n",
    "tokenized = [preprocess(m) for m in messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "# TODO: Implement \n",
    "\n",
    "all_tweets = lambda t: [twt for tokenized in t for twt in tokenized]\n",
    "all_tweets = all_tweets(tokenized)\n",
    "\n",
    "bow = Counter(all_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out sample tokenized word\n",
    "\n",
    "bow['arrow']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "\n",
    "remove frequent and rare words:\n",
    "\n",
    "stop words 'It', 'the', 'and' add little to the sense of a piece of text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 398753), ('to', 379487), ('is', 284865), ('for', 273537), ('on', 241663), ('of', 211334), ('and', 208471), ('in', 205307), ('this', 203540), ('it', 193484), ('at', 138453), ('will', 128180), ('up', 121567), ('are', 101424), ('you', 94275), ('that', 89655)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22858"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "total_count = len(bow)\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "freqs = {word: count/total_count for word, count in bow.items()}\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.00007\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 16\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = bow.most_common(n=high_cutoff)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(K_most_common)\n",
    "len(filtered_words) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words\n",
    "Let's creat three variables that will help with our vocabulary.\n",
    "\n",
    "### A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "                                # Starting the list at 1   >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "                                # Starting the list at 1   >\n",
    "vocab = {word:ii for ii, word in enumerate(filtered_words, 1)}\n",
    "\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in enumerate(filtered_words, 1)}\n",
    "\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "filtered = [[word for word in message if word in vocab] for message in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'buy', 'at', 'ill', 'wait'], [], ['staanalystalert', 'for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'at', 'usd', 'our', 'own', 'verdict', 'is', 'buy'], ['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'on', 'stocktwits'], ['reveal', 'yourself'], ['why', 'the', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position'], ['bear', 'have', 'reason', 'on', 'to', 'pay', 'more', 'attention'], ['ok', 'good', 'we', 're', 'not', 'dropping', 'in', 'price', 'over', 'the', 'weekend', 'lol'], ['daily', 'chart', 'we', 'need', 'to', 'get', 'back', 'to', 'above'], ['drop', 'per', 'week', 'after', 'spike', 'if', 'no', 'news', 'in', 'month', 'back', 'to', 'if', 'bo', 'then', 'bingo', 'what', 'is', 'the', 'odds']]\n"
     ]
    }
   ],
   "source": [
    "print(filtered[0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "\n",
    "\n",
    "since 50% of twits are labeled as neutral this will make the model inherently 50%\n",
    "accurate if it just predicted neutral constantly.\n",
    "\n",
    "As such we rebalance to get a more uniformly distributed sample of stock twits.\n",
    "\n",
    "This will enable the model to train more evenly on different sentiments.\n",
    "\n",
    "We will also apply a dropout inorder to get the model to not overfit and to make sure all tensor nodes are training and prevent the model relying too heavily on certain nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of neutral setimenets are: 701597\n",
      "\n",
      "the number of total sentiments in our sample is: 1548010\n",
      "\n",
      "Neutral sentiments make up 45 percent of the sample population of twits\n",
      "\n",
      "The probability used to reduce the amount of neutral words is: 30 percent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "print('the number of neutral setimenets are: {}\\n'.format(n_neutral))\n",
    "# 2 = Neutral sentiment\n",
    "# here we get the total number of sentiments to test against\n",
    "N_examples = len(sentiments)\n",
    "print('the number of total sentiments in our sample is: {}\\n'.format(N_examples))\n",
    "\n",
    "print('Neutral sentiments make up {} percent of the sample population of twits\\n'.format(round((n_neutral/N_examples)*100)))\n",
    "\n",
    "# what probability to use to get down to only circa 20% neutral twits vs 50% initially\n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "# The keep probability for reducing the neutrak sentiments down to circa 20%\n",
    "print('The probability used to reduce the amount of neutral words is: {} percent\\n'.format(round(keep_prob*100)))\n",
    "\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "        \n",
    "        # If the tweet sentiment is random either automatically reject to add to our messages and setiments\n",
    "        # or if the random.random value is less than keep prob then we allow the neutral word to proceed and be added.\n",
    "        \n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        \n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'buy', 'at', 'ill', 'wait'], ['staanalystalert', 'for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'at', 'usd', 'our', 'own', 'verdict', 'is', 'buy'], ['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'on', 'stocktwits'], ['why', 'the', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position'], ['bear', 'have', 'reason', 'on', 'to', 'pay', 'more', 'attention'], ['ok', 'good', 'we', 're', 'not', 'dropping', 'in', 'price', 'over', 'the', 'weekend', 'lol'], ['daily', 'chart', 'we', 'need', 'to', 'get', 'back', 'to', 'above'], ['drop', 'per', 'week', 'after', 'spike', 'if', 'no', 'news', 'in', 'month', 'back', 'to', 'if', 'bo', 'then', 'bingo', 'what', 'is', 'the', 'odds'], ['strong', 'buy'], ['short', 'ratio', 'is', 'at', 'and', 'short', 'to', 'float', 'is', 'via']]\n",
      "[4, 4, 3, 3, 0, 3, 4, 0, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "print(balanced['messages'][0:10])\n",
    "print(balanced['sentiments'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1950604569886999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert our word tokens into integers that we can feed into the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the vocab method to each word in each message within each message block in the balanced words dataset\n",
    "\n",
    "# i.e. the balanced words dataset has the amount of neutral message blocks reduced to circa 20%\n",
    "\n",
    "token_ids =[[vocab[word] for word in message] for message in balanced['messages']]\n",
    "\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Now define our Network\n",
    "\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Implement the text classifier\n",
    "\n",
    "The RNN network model named \"TextClassifer\" - consists of three main parts: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "For this network's `forward` pass, we use softmax instead of sigmoid. The reason we are not using sigmoid is that the output of NN is not a binary we are doing multiclass predictions with a probability distributiob. \n",
    "\n",
    "In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of using one hot encoding, due to the size of the corpus we'll end up with a huge one hot input vector of mostly zero's.\n",
    "\n",
    "### As such its more efficient to use a hidden layer of our embedding layer \n",
    "\n",
    "### From the embedding layer we can use it as a lookup table in unison with the one hot input vector to get the embedding layers particular weights for that word.\n",
    "\n",
    "\n",
    "#### The important part is that if two words are similar in their contexts (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. \n",
    "\n",
    "![alt text](https://github.com/Pytrader1x/AI_for_trading/blob/master/Cosine.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words!\n",
    "\n",
    "#### You can use the cosine similarity to identify the distance between words and plot this for the whole sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3a5a10e3d6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \"\"\"\n\u001b[1;32m      4\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mby\u001b[0m \u001b[0msetting\u001b[0m \u001b[0mup\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers,\n",
    "                              dropout=dropout,batch_first=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # we want the model to be fully connected so the LSTM layer outputs to every class we're predicting\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        # using the generator\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # As we go throug mini batches we dont want to mix up gradients so we zero them out\n",
    "        hidden_state = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                       weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        \n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        nn_input = nn_input.long()\n",
    "        \n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # drop a certain percentage of the tensors to improve model training and not over train \n",
    "        # certain tensors more than others\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # use the logsoftmax (created above)mmore efficient to get the probabilities out for each class\n",
    "        logps = self.softmax(out)           \n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4646, -1.5817, -1.7470, -1.4167, -1.9215],\n",
      "        [-1.4673, -1.6021, -1.7564, -1.3891, -1.9240],\n",
      "        [-1.4453, -1.5940, -1.7918, -1.3855, -1.9355],\n",
      "        [-1.4634, -1.6071, -1.7193, -1.4239, -1.9096]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            # for each batch of messages get the length of sequence - the len of this token tensor\n",
    "            #in order to generate the start index\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031229\n",
      "1031229\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "features_train_test_split = 0.8\n",
    "\n",
    "\n",
    "# if features_train_test_split != labels_train_test_split:\n",
    "#     print('features and labels arent same percentage change back to be same')\n",
    "\n",
    "#     break\n",
    "    \n",
    "    \n",
    "# TODO Implement \n",
    "# FEATURES\n",
    "\n",
    "\n",
    "\n",
    "train_token_percent = int(len(token_ids) * features_train_test_split)\n",
    "\n",
    "\n",
    "train_features = token_ids[0:train_token_percent]\n",
    "valid_features = token_ids[train_token_percent:]\n",
    "\n",
    "\n",
    "# LABELS\n",
    "train_label_percent = int(len(sentiments) * features_train_test_split)\n",
    "\n",
    "train_labels = sentiments[0:train_label_percent]\n",
    "valid_labels = sentiments[train_label_percent:]\n",
    "\n",
    "print(len(sentiments))\n",
    "print(len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features amount to 824983\n",
      "validation features amount to 206246\n",
      "the total features tokenids are 1031229\n",
      "\n",
      "train labels amount to 824983\n",
      "validation labels amount to 206246\n",
      "the total sentiment labels are 1031229\n",
      " \n",
      "The total sentiment labels split between train and validation \n",
      "amount to the input sentiments labels total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('train features amount to {}'.format(len(train_features)))\n",
    "print('validation features amount to {}'.format(len(valid_features)))\n",
    "print('the total features tokenids are {}\\n'.format(len(train_features)+len(valid_features)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('train labels amount to {}'.format(len(train_labels)))\n",
    "print('validation labels amount to {}'.format(len(valid_labels)))\n",
    "\n",
    "print('the total sentiment labels are {}\\n '.format(len(train_labels)+len(valid_labels)))\n",
    "\n",
    "\n",
    "if (len(train_labels)+len(valid_labels)) ==(len(sentiments)):\n",
    "    print('The total sentiment labels split between train and validation \\namount to the input sentiments labels total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(22859, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=3, dropout=0.4)\n",
       "  (dropout): Dropout(p=0.4)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=3, dropout=0.40)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/pytrader1x/ai-for-trading/20351e2056eb44299800a2f4229b2c93\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch 1/6... Step: 100... Train Loss: 0.939997... Valid Loss: 1.002226 Accuracy: 0.597931 Time: 60.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type TextClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy model is saved\n",
      "Epoch 1/6... Step: 200... Train Loss: 0.868307... Valid Loss: 0.940318 Accuracy: 0.625146 Time: 60.580\n",
      "New best accuracy model is saved\n",
      "Epoch 1/6... Step: 300... Train Loss: 0.816581... Valid Loss: 0.900149 Accuracy: 0.644832 Time: 60.685\n",
      "New best accuracy model is saved\n",
      "Epoch 1/6... Step: 400... Train Loss: 0.828916... Valid Loss: 0.868944 Accuracy: 0.655995 Time: 60.519\n",
      "New best accuracy model is saved\n",
      "Epoch 1/6... Step: 500... Train Loss: 0.790067... Valid Loss: 0.869217 Accuracy: 0.656466 Time: 60.771\n",
      "New best accuracy model is saved\n",
      "Epoch: 46.69570565223694\n",
      "Starting epoch 2\n",
      "Epoch 2/6... Step: 100... Train Loss: 0.712329... Valid Loss: 0.860576 Accuracy: 0.665520 Time: 60.847\n",
      "New best accuracy model is saved\n",
      "Epoch 2/6... Step: 200... Train Loss: 0.721254... Valid Loss: 0.823752 Accuracy: 0.683375 Time: 60.821\n",
      "New best accuracy model is saved\n",
      "Epoch 2/6... Step: 300... Train Loss: 0.719624... Valid Loss: 0.811143 Accuracy: 0.686815 Time: 60.565\n",
      "New best accuracy model is saved\n",
      "Epoch 2/6... Step: 400... Train Loss: 0.655388... Valid Loss: 0.879926 Accuracy: 0.663448 Time: 60.825\n",
      "Epoch 2/6... Step: 500... Train Loss: 0.651941... Valid Loss: 0.899467 Accuracy: 0.656889 Time: 60.852\n",
      "Epoch: 46.4017071723938\n",
      "Starting epoch 3\n",
      "Epoch 3/6... Step: 100... Train Loss: 0.599214... Valid Loss: 0.813523 Accuracy: 0.687354 Time: 60.700\n",
      "New best accuracy model is saved\n",
      "Epoch 3/6... Step: 200... Train Loss: 0.662444... Valid Loss: 0.799047 Accuracy: 0.694596 Time: 60.800\n",
      "New best accuracy model is saved\n",
      "Epoch 3/6... Step: 300... Train Loss: 0.668737... Valid Loss: 0.795244 Accuracy: 0.693746 Time: 60.674\n",
      "Epoch 3/6... Step: 400... Train Loss: 0.639021... Valid Loss: 0.808664 Accuracy: 0.689324 Time: 60.736\n",
      "Epoch 3/6... Step: 500... Train Loss: 0.651394... Valid Loss: 0.797771 Accuracy: 0.696311 Time: 60.705\n",
      "New best accuracy model is saved\n",
      "Epoch: 46.64769649505615\n",
      "Starting epoch 4\n",
      "Epoch 4/6... Step: 100... Train Loss: 0.601020... Valid Loss: 0.818727 Accuracy: 0.692473 Time: 60.794\n",
      "Epoch 4/6... Step: 200... Train Loss: 0.639897... Valid Loss: 0.826883 Accuracy: 0.695572 Time: 60.849\n",
      "Epoch 4/6... Step: 300... Train Loss: 0.589974... Valid Loss: 0.829888 Accuracy: 0.688751 Time: 60.719\n",
      "Epoch 4/6... Step: 400... Train Loss: 0.635061... Valid Loss: 0.804885 Accuracy: 0.696598 Time: 60.893\n",
      "New best accuracy model is saved\n",
      "Epoch 4/6... Step: 500... Train Loss: 0.565294... Valid Loss: 0.796106 Accuracy: 0.700514 Time: 60.503\n",
      "New best accuracy model is saved\n",
      "Epoch: 46.64722919464111\n",
      "Starting epoch 5\n",
      "Epoch 5/6... Step: 100... Train Loss: 0.566329... Valid Loss: 0.832211 Accuracy: 0.696551 Time: 60.722\n",
      "Epoch 5/6... Step: 200... Train Loss: 0.596192... Valid Loss: 0.819379 Accuracy: 0.700431 Time: 60.674\n",
      "Epoch 5/6... Step: 300... Train Loss: 0.550626... Valid Loss: 0.792255 Accuracy: 0.705496 Time: 60.708\n",
      "New best accuracy model is saved\n",
      "Epoch 5/6... Step: 400... Train Loss: 0.566299... Valid Loss: 0.839539 Accuracy: 0.694603 Time: 60.580\n",
      "Epoch 5/6... Step: 500... Train Loss: 0.558930... Valid Loss: 0.820320 Accuracy: 0.693272 Time: 60.598\n",
      "Epoch: 46.41813135147095\n",
      "Starting epoch 6\n",
      "Epoch 6/6... Step: 100... Train Loss: 0.522691... Valid Loss: 0.882346 Accuracy: 0.684381 Time: 60.758\n",
      "Epoch 6/6... Step: 200... Train Loss: 0.536716... Valid Loss: 0.894825 Accuracy: 0.686774 Time: 60.813\n",
      "Epoch 6/6... Step: 300... Train Loss: 0.561042... Valid Loss: 0.872647 Accuracy: 0.691222 Time: 60.672\n",
      "Epoch 6/6... Step: 400... Train Loss: 0.600696... Valid Loss: 0.844155 Accuracy: 0.699880 Time: 60.739\n",
      "Epoch 6/6... Step: 500... Train Loss: 0.548247... Valid Loss: 0.861185 Accuracy: 0.692891 Time: 60.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/pytrader1x/ai-for-trading/20351e2056eb44299800a2f4229b2c93\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     Validation accuracy [12060]   : (0.583008, 0.754883)\n",
      "COMET INFO:     curr_epoch [6]                : 5\n",
      "COMET INFO:     loss [354]                    : (0.4813113808631897, 1.6096088886260986)\n",
      "COMET INFO:     sys.gpu.0.free_memory [59]    : (9291825152.0, 11490689024.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [59]: (0.0, 100.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory        : (11996954624.0, 11996954624.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [59]    : (506265600.0, 2705129472.0)\n",
      "COMET INFO: ----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46.44739651679993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "# Comet ML\n",
    "experiment = Experiment(api_key=\"key\",\n",
    "                        project_name=\"key\", workspace=\"key\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0008\n",
    "best_val_accuracy = 0\n",
    "clip = 5 # Gradient clipping to \n",
    "# we use clip grad norm since\n",
    "# clip: The maximum gradient value to clip at (to prevent exploding gradients)\n",
    "print_every = 100\n",
    "\n",
    "hyper_params = {\n",
    "    \"sequence_length\": 20,\n",
    "    \"input_size\": 512,\n",
    "    \"dropout\": 0.40,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_size\": 1024,\n",
    "    \"num_epochs\": 6,\n",
    "    \"learning_rate\": 0.0008\n",
    "}\n",
    "\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    experiment.log_current_epoch(epoch)\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        #Once at 600 steps break to next batch\n",
    "        if steps == 600:\n",
    "            break\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        \n",
    "        \n",
    "        \n",
    "        # we backpropegate through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        #Zero out the gradients as we back propegate through the network\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #get tuple out of the model for backward pass for log probs and the hidden layer\n",
    "        logps, hidden = model(text_batch, hidden)\n",
    "        \n",
    "        loss = criterion(logps, labels)\n",
    "        #Backward pass loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # we use clip grad norm since\n",
    "        # clip: The maximum gradient value to clip at (to prevent exploding gradients)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # Take one step using the updated values from the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            start = time()\n",
    "            model.eval()\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            \n",
    "            # perform validation using init hidden function\n",
    "            valid_hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            valid_losses = []\n",
    "            accuracy = []\n",
    "            \n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size, \n",
    "                                                 sequence_length=20, shuffle = True):\n",
    "                \n",
    "                if text_batch.size(1) != batch_size:\n",
    "                    break\n",
    "                for each in hidden:\n",
    "                    # perform validation on the devide cpu / GPU also for performance / continuity\n",
    "                    each.to(device)\n",
    "                    # perform the backpropegation\n",
    "                    valid_hidden = tuple([each.data for each in valid_hidden]) \n",
    "                    \n",
    "                    text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                    \n",
    "                    #use the model to derive the validation loop probabilities and its hidden layer\n",
    "                    valid_logps, valid_hidden = model(text_batch, valid_hidden)\n",
    "                    \n",
    "                    valid_loss = criterion(valid_logps.squeeze(), labels.long())\n",
    "                    \n",
    "                    #add this batch add the loss to the running measure of validation loss\n",
    "                    valid_losses.append(valid_loss.item()) \n",
    "            \n",
    "                    #Accuracy\n",
    "                    # to move from soft max output probs to actual probabilities \n",
    "                    # we take the exponent of validation loss\n",
    "                    ps = torch.exp(valid_logps)\n",
    "                    \n",
    "                    # get tuple out from ps tensor output along the columns, retrieve the 1st highest one\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    \n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "                    \n",
    "                    # Log to Comet.ml\n",
    "                    #rounded_accuracy = np.mean(accuracy)\n",
    "                    #rounded_accuracy = round(accuracy, 5)\n",
    "                    #comet ai log accuracy\n",
    "                    #experiment.log_metric(\"accuracy\", rounded_accuracy, step=steps)\n",
    "                    outPut_accuracy = round(np.mean(accuracy), 6)\n",
    "                    experiment.log_metric(\"Validation accuracy\", outPut_accuracy, step=steps)\n",
    "            \n",
    "            end_time_ = time()\n",
    "            model.train()\n",
    "            \n",
    "            current_val_accuracy = sum(accuracy)/len(accuracy)\n",
    "            \n",
    "            # print out running values\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs)\n",
    "                    ,'Step: {}...'.format(steps)\n",
    "                  ,'Train Loss: {:.6f}...'.format(loss.item()),\n",
    "                 'Valid Loss: {:.6f}'.format(np.mean(valid_losses)),\n",
    "                 'Accuracy: {:.6f}'.format(np.mean(accuracy)),\n",
    "                 'Time: {:.3f}'.format(end_time_-start))\n",
    "            \n",
    "            if current_val_accuracy > best_val_accuracy:\n",
    "                \n",
    "                torch.save(model,'p6_epoch2.pth')\n",
    "                best_val_accuracy=current_val_accuracy\n",
    "                print(\"New best accuracy model is saved\")\n",
    "        \n",
    "    \n",
    "            \n",
    "    print('Epoch: {}'.format((time()-end_time_)))  \n",
    "            \n",
    "        \n",
    "#Conet end\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over the first model\n",
    "\n",
    "\n",
    "### Changing hyperparameters evenetually will create a loop to change these hyperparamets within a specified range using Comet.ml\n",
    "\n",
    "### Learning rate\n",
    "### LSTM layers\n",
    "### Batch size\n",
    "### Dropout to prevent overfitting\n",
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(22859, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=3, dropout=0.35)\n",
       "  (dropout): Dropout(p=0.35)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=3, dropout=0.35)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/pytrader1x/ai-for-trading/602a7986374a41e48bad189ffbad64e4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch 1/6... Step: 100... Train Loss: 0.965420... Valid Loss: 1.057349 Accuracy: 0.572147 Time: 60.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type TextClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy model is saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2ce7d8fbf4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# we use clip grad norm since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# clip: The maximum gradient value to clip at (to prevent exploding gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Take one step using the updated values from the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "# Comet ML\n",
    "experiment = Experiment(api_key=\"key\",\n",
    "                        project_name=\"key\", workspace=\"key\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0005\n",
    "best_val_accuracy = 0\n",
    "clip = 5 # Gradient clipping to \n",
    "# we use clip grad norm since\n",
    "# clip: The maximum gradient value to clip at (to prevent exploding gradients)\n",
    "print_every = 100\n",
    "\n",
    "hyper_params = {\n",
    "    \"sequence_length\": 20,\n",
    "    \"input_size\": 512,\n",
    "    \"dropout\": 0.35,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_size\": 1024,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.0005\n",
    "}\n",
    "\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    experiment.log_current_epoch(epoch)\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        #Once at 600 steps break to next batch\n",
    "        if steps == 600:\n",
    "            break\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        \n",
    "        \n",
    "        \n",
    "        # we backpropegate through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        #Zero out the gradients as we back propegate through the network\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #get tuple out of the model for backward pass for log probs and the hidden layer\n",
    "        logps, hidden = model(text_batch, hidden)\n",
    "        \n",
    "        loss = criterion(logps, labels)\n",
    "        #Backward pass loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # we use clip grad norm since\n",
    "        # clip: The maximum gradient value to clip at (to prevent exploding gradients)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # Take one step using the updated values from the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            start = time()\n",
    "            model.eval()\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            \n",
    "            # perform validation using init hidden function\n",
    "            valid_hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            valid_losses = []\n",
    "            accuracy = []\n",
    "            \n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size, \n",
    "                                                 sequence_length=20, shuffle = True):\n",
    "                \n",
    "                if text_batch.size(1) != batch_size:\n",
    "                    break\n",
    "                for each in hidden:\n",
    "                    # perform validation on the devide cpu / GPU also for performance / continuity\n",
    "                    each.to(device)\n",
    "                    # perform the backpropegation\n",
    "                    valid_hidden = tuple([each.data for each in valid_hidden]) \n",
    "                    \n",
    "                    text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                    \n",
    "                    #use the model to derive the validation loop probabilities and its hidden layer\n",
    "                    valid_logps, valid_hidden = model(text_batch, valid_hidden)\n",
    "                    \n",
    "                    valid_loss = criterion(valid_logps.squeeze(), labels.long())\n",
    "                    \n",
    "                    #add this batch add the loss to the running measure of validation loss\n",
    "                    valid_losses.append(valid_loss.item()) \n",
    "            \n",
    "                    #Accuracy\n",
    "                    # to move from soft max output probs to actual probabilities \n",
    "                    # we take the exponent of validation loss\n",
    "                    ps = torch.exp(valid_logps)\n",
    "                    \n",
    "                    # get tuple out from ps tensor output along the columns, retrieve the 1st highest one\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    \n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "                    \n",
    "                    # Log to Comet.ml\n",
    "                    #rounded_accuracy = np.mean(accuracy)\n",
    "                    #rounded_accuracy = round(accuracy, 5)\n",
    "                    #comet ai log accuracy\n",
    "                    #experiment.log_metric(\"accuracy\", rounded_accuracy, step=steps)\n",
    "                    outPut_accuracy = round(np.mean(accuracy), 6)\n",
    "                    experiment.log_metric(\"Validation accuracy\", outPut_accuracy, step=steps)\n",
    "            \n",
    "            end_time_ = time()\n",
    "            model.train()\n",
    "            \n",
    "            current_val_accuracy = sum(accuracy)/len(accuracy)\n",
    "            \n",
    "            # print out running values\n",
    "            print('Epoch {}/{}...'.format(epoch+1, epochs)\n",
    "                    ,'Step: {}...'.format(steps)\n",
    "                  ,'Train Loss: {:.6f}...'.format(loss.item()),\n",
    "                 'Valid Loss: {:.6f}'.format(np.mean(valid_losses)),\n",
    "                 'Accuracy: {:.6f}'.format(np.mean(accuracy)),\n",
    "                 'Time: {:.3f}'.format(end_time_-start))\n",
    "            \n",
    "            if current_val_accuracy > best_val_accuracy:\n",
    "                \n",
    "                torch.save(model,'p6_epoch2.pth')\n",
    "                best_val_accuracy=current_val_accuracy\n",
    "                print(\"New best accuracy model is saved\")\n",
    "        \n",
    "    \n",
    "            \n",
    "    print('Epoch: {}'.format((time()-end_time_)))  \n",
    "            \n",
    "        \n",
    "#Conet end\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction \n",
    "\n",
    "the `predict` function to generate the prediction vector from a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [value for value in tokens if value in vocab]\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[x] for x in tokens] \n",
    "        \n",
    "    # Adding a batch dimension resizing using view\n",
    "    text_input = torch.from_numpy(np.asarray(torch.FloatTensor(tokens).view(-1, 1))) \n",
    "    \n",
    "    # Get the netowork output hidden and log probabilities to then get the exponent of.\n",
    "    hidden = model.init_hidden(1)\n",
    "    logps, _ = model.forward(text_input, hidden)\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0022,  0.0275,  0.0213,  0.4866,  0.4625]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "** TODO: Answer Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[ 0.1693,  0.1346,  0.1764,  0.2188,  0.3008]]),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
